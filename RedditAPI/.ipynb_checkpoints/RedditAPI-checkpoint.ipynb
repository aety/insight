{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/scraping-reddit-data-1c0af3040768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id='sN5rLXnScb8Ffg', client_secret='hwJ6C7WMcpIZzFhXMhfT1hHzRVA', user_agent='instanthot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regarding beginner's guides\n",
      "[D] Machine Learning - WAYR (What Are You Reading) - Week 70\n",
      "[R] DeepPrivacy: A Generative Adversarial Network for Face Anonymization\n",
      "[D] Batch Normalization is a Cause of Adversarial Vulnerability\n",
      "[P] SpeechBrain: A PyTorch-based Speech Toolkit.\n",
      "[R] On Extractive and Abstractive Neural Document Summarization with Transformer Language Models\n",
      "[D] How much do CS/ML assistant professors get paid?\n",
      "[R] Automatic Critical Mechanic Discovery in Video Games\n",
      "[P] Fine-Tuned GPT-2 to generate new plots form IMDB's top 250 movies\n",
      "[R] The funniest game against DeepMind's AlphaStar thus far\n"
     ]
    }
   ],
   "source": [
    "# get 10 hot posts from the MachineLearning subreddit\n",
    "hot_posts = reddit.subreddit('MachineLearning').hot(limit=10)\n",
    "for post in hot_posts:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumb ass teachers\n",
      "PeaCOCKS\n",
      "Rick Scott administration rejected federal funds to fight HIV, sending Florida infection rates soaring\n",
      "Another plot point\n",
      "TIL that deaths from 9/11 related illnesses will soon outpace the number of people lost on that fateful day.\n",
      "Kingfisher diving into a pond\n",
      "The sentiment in Scotland is one I fully agree with.\n",
      "We meet again\n",
      "*Dramatic music plays*\n",
      "Rise of the fallen. [OC]\n"
     ]
    }
   ],
   "source": [
    "# get hottest posts from all subreddits\n",
    "hot_posts = reddit.subreddit('all').hot(limit=10)\n",
    "for post in hot_posts:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  score      id  \\\n",
      "0                        Regarding beginner's guides    493  co37ut   \n",
      "1  [D] Machine Learning - WAYR (What Are You Read...     93  d1g1k9   \n",
      "2  [R] DeepPrivacy: A Generative Adversarial Netw...     87  d2iv9w   \n",
      "3  [D] Batch Normalization is a Cause of Adversar...     38  d2m5zr   \n",
      "4   [P] SpeechBrain: A PyTorch-based Speech Toolkit.    220  d286fp   \n",
      "5  [R] On Extractive and Abstractive Neural Docum...     29  d2i9pn   \n",
      "6  [D] How much do CS/ML assistant professors get...      3  d2l70d   \n",
      "7  [R] Automatic Critical Mechanic Discovery in V...      3  d2m1hh   \n",
      "8  [P] Fine-Tuned GPT-2 to generate new plots for...      1  d2pjhu   \n",
      "9  [R] The funniest game against DeepMind's Alpha...     13  d29byz   \n",
      "\n",
      "         subreddit                                                url  \\\n",
      "0  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "1  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "2  MachineLearning                   https://arxiv.org/abs/1909.04538   \n",
      "3  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "4  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "5  MachineLearning                   https://arxiv.org/abs/1909.03186   \n",
      "6  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "7  MachineLearning                   https://arxiv.org/abs/1909.03094   \n",
      "8  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "9  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
      "\n",
      "   num_comments                                               body  \\\n",
      "0            52  Hi all,\\n\\n\\n/r/machinelearning is growing ram...   \n",
      "1            27  This is a place to share machine learning rese...   \n",
      "2             6                                                      \n",
      "3            13  Abstract - Batch normalization (batch norm) is...   \n",
      "4            44  Hi there!\\n\\nWe are happy to announce the Spee...   \n",
      "5             3                                                      \n",
      "6             9  While reading the other thread ([https://www.r...   \n",
      "7             1                                                      \n",
      "8             0  Hello r/MachineLearning,\\n\\n[\\>Link to the Twi...   \n",
      "9            21  Human players are starting to get creative in ...   \n",
      "\n",
      "        created  \n",
      "0  1.565390e+09  \n",
      "1  1.568002e+09  \n",
      "2  1.568197e+09  \n",
      "3  1.568214e+09  \n",
      "4  1.568154e+09  \n",
      "5  1.568194e+09  \n",
      "6  1.568208e+09  \n",
      "7  1.568213e+09  \n",
      "8  1.568235e+09  \n",
      "9  1.568159e+09  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "posts = []\n",
    "ml_subreddit = reddit.subreddit('MachineLearning')\n",
    "for post in ml_subreddit.hot(limit=10):\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "print(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\n",
      "--------\n",
      "+[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\n",
      "--------\n",
      "+[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\n",
      "--------\n",
      "+[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AProject)\n",
      "--------\n",
      "+[News](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ANews)\n",
      "--------\n",
      "***[@slashML on Twitter](https://twitter.com/slashML)***\n",
      "--------\n",
      "***[Chat with us on Slack](https://join.slack.com/t/rml-talk/shared_invite/enQtNjkyMzI3NjA2NTY2LWY0ZmRjZjNhYjI5NzYwM2Y0YzZhZWNiODQ3ZGFjYmI2NTU3YjE1ZDU5MzM2ZTQ4ZGJmOTFmNWVkMzFiMzVhYjg)***\n",
      "--------\n",
      "**Beginners:**\n",
      "--------\n",
      "Please have a look at [our FAQ and Link-Collection](http://www.reddit.com/r/MachineLearning/wiki/index)\n",
      "\n",
      "[Metacademy](http://www.metacademy.org) is a great resource which compiles lesson plans on popular machine learning topics.\n",
      "\n",
      "For Beginner questions please try /r/LearnMachineLearning , /r/MLQuestions or http://stackoverflow.com/\n",
      "\n",
      "For career related questions, visit /r/cscareerquestions/\n",
      "\n",
      "--------\n",
      "\n",
      "[Advanced Courses](https://www.reddit.com/r/MachineLearning/comments/51qhc8/phdlevel_courses?st=isz2lqdk&sh=56c58cd6)\n",
      "\n",
      "--------\n",
      "**AMAs:**\n",
      "\n",
      "[Pluribus Poker AI Team 7/19/2019](https://www.reddit.com/r/MachineLearning/comments/ceece3/ama_we_are_noam_brown_and_tuomas_sandholm/)\n",
      "\n",
      "[DeepMind AlphaStar team (1/24//2019)](https://www.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/)\n",
      "\n",
      "[Libratus Poker AI Team (12/18/2017)]\n",
      "(https://www.reddit.com/r/MachineLearning/comments/7jn12v/ama_we_are_noam_brown_and_professor_tuomas/)\n",
      "\n",
      "[DeepMind AlphaGo Team (10/19/2017)](https://www.reddit.com/r/MachineLearning/comments/76xjb5/ama_we_are_david_silver_and_julian_schrittwieser/)\n",
      "\n",
      "[Google Brain Team (9/17/2017)](https://www.reddit.com/r/MachineLearning/comments/6z51xb/we_are_the_google_brain_team_wed_love_to_answer/)\n",
      "\n",
      "[Google Brain Team (8/11/2016)]\n",
      "(https://www.reddit.com/r/MachineLearning/comments/4w6tsv/ama_we_are_the_google_brain_team_wed_love_to/)\n",
      "\n",
      "[The MalariaSpot Team (2/6/2016)](https://www.reddit.com/r/MachineLearning/comments/4m7ci1/ama_the_malariaspot_team/)\n",
      "\n",
      "[OpenAI Research Team (1/9/2016)](http://www.reddit.com/r/MachineLearning/comments/404r9m/ama_the_openai_research_team/)\n",
      "\n",
      "[Nando de Freitas (12/26/2015)](http://www.reddit.com/r/MachineLearning/comments/3y4zai/ama_nando_de_freitas/)\n",
      "\n",
      "[Andrew Ng and Adam Coates (4/15/2015)](http://www.reddit.com/r/MachineLearning/comments/32ihpe/ama_andrew_ng_and_adam_coates/)\n",
      "\n",
      "[JÃ¼rgen Schmidhuber (3/4/2015)](http://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/)\n",
      "\n",
      "[Geoffrey Hinton (11/10/2014)]\n",
      "(http://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/)\n",
      "\n",
      "[Michael Jordan (9/10/2014)](http://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/)\n",
      "\n",
      "[Yann LeCun (5/15/2014)](http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/)\n",
      "\n",
      "[Yoshua Bengio (2/27/2014)](http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio/)\n",
      "\n",
      "--------\n",
      "Related Subreddit :\n",
      "\n",
      "* [LearnMachineLearning](http://www.reddit.com/r/LearnMachineLearning)\n",
      "\n",
      "* [Statistics](http://www.reddit.com/r/statistics)\n",
      "\n",
      "* [Computer Vision](http://www.reddit.com/r/computervision)\n",
      "\n",
      "* [Compressive Sensing](http://www.reddit.com/r/CompressiveSensing/)\n",
      "\n",
      "* [NLP] (http://www.reddit.com/r/LanguageTechnology)\n",
      "\n",
      "* [ML Questions] (http://www.reddit.com/r/MLQuestions)\n",
      "\n",
      "* /r/MLjobs and /r/BigDataJobs\n",
      "\n",
      "* /r/datacleaning\n",
      "\n",
      "* /r/DataScience\n",
      "\n",
      "* /r/scientificresearch\n",
      "\n",
      "* /r/artificial\n"
     ]
    }
   ],
   "source": [
    "# get MachineLearning subreddit data\n",
    "ml_subreddit = reddit.subreddit('MachineLearning')\n",
    "\n",
    "print(ml_subreddit.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comments from a specific post\n",
    "submission = reddit.submission(url=\"https://www.reddit.com/r/MapPorn/comments/a3p0uq/an_image_of_gps_tracking_of_multiple_wolves_in/\")\n",
    "# or \n",
    "submission = reddit.submission(id=\"a3p0uq\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
